1. eksctl create cluster -f eks.yaml

2. ekstl scale nodegroup --cluster=eks-demo --name=eks-ng --nodes=2 --nodes-min=2 --nodes-max=2 --region us-east-1

3. create identity provider:
> eksctl utils asociate-iam-oidc-provider --name eks-demo --region us-east-1 --approve

4. create a serviceaccount: create iam role and serviceaccount and map it together.
> eksctl create iamserviceaccount --name sa-poc --cluster eks-demo --attach-policy-arn <aws policy arn> --approve --region us-east-1 --namespace dev

5. check serviceaccount created:
> kubectl get sa

7. assign eks access to new user:
   a) create a new aws iam user.
   b) Run aws configure --profile eksdev
   c) Generate kubeconfig for eks cluster:
       > aws eks update-kubeconfig --name eks-demo --role-arn <role-attached with workspace during cluster creation because its there in RBAC> --profile eksdev
   d) Establish trust between user created on laptop and above mentioned role.

8. To get EKS endpoint
> aws eks describe-cluster --name eks-demo --region us-east-1
> Setup bastian and localport forwarding to access cluster and make api-endpoint as "private" in eks cluster. Also don't forget to configure api endpont in /etc/host file.
> Also in cluster "Additional security group" allow all traffic from jumpbox security group.

9. To install calico, first need delete nodegroup and then delete aws-node pod. Then install calico plugin and add nodegroup again.
> eksctl delete nodegroup --cluster eks-demo --name eks-ng --region us-east-1
> kubectl get ds -n kube-system (deamonset with aws-node, delete it)
> kubectl delete ds aws-node -n kube-system
> kubectl apply -f calico_cni.yaml    (Install calico plugin)
> kubectl get ds -n kube-system (you will see calico-cni deamonset)
> eksctl create nodegroup --cluster eks-demo --name eks-ng --node-type t3.small --nodes 1 --node-ami auto --max-pods-per-node 100 --node-private-networking --region us-east-1 (--max-pods-per-node using this oprion is important else pods will be created using subenet, vpc ip address range, not internal ips. Also using calico will create unmanaged nodegroup)

10.Install calicocli:
> curl -O -L  https://github.com/projectcalico/calicoctl/releases/download/v3.18.1/calicoctl
> chmod +x calicoctl
> sudo mv calicoctl /usr/local/bin

11. Configure calicoctl to connect to the Kubernetes API datastore
> export DATASTORE_TYPE=kubernetes
> export KUBECONFIG=~/.kube/config
> calicoctl version
> calicoctl get nodes --output wide (list nodes)
> calicoctl ipam show  (show internal IP access management)
> calicoctl apply -f pool2.yaml (create new ip pool )
> calicoctl get ippool -o wide (you will see 2 ippool, so lets disable default ippool)
> calicoctl get ippool -o yaml > pool.yaml  (disable default pool using disabled: true config in pool.yaml)
> calicoctl apply -f pool.yaml
> calicoctl get ippool -o wide (you will see default ippool is disabled)
> calicocli delete pool default-ipv4-ippool (delete default ippool)

12. AWS KMS keys to encrypt secrets: Whenever you create a secret that info is stored in etcd datastore. Those secrets are encrypted by AWS managed encryption keys and we don't have ay access on those keys.
    EKS allows us to encrypt data which is already encrypted by AWS managed keys, using our own keys.Thats why its called envelpe encryption because we are providing encryption on top of already encrypted secret.
    > create a customer managed key in KMS
    > Copy key arn
    > In eks.yaml, under secretsEncryption section add arn.
    > Make sure to create a key during cluster creation, else later it can't be changed.

13. Pod Security Policy: It defines a set of conditions that a pod must run with in order to be accepted in a system or in order to be scheduled in any of the node.In eks we have one default PSP and its cluster wide resource.
    > kubectl get PSP
    > EKS default policy doesn't apply any restriction in pod. Whenever we run a pod its validated against PSP.
    > kubectl describe psp eks.privileged
    > kubectl delete psp eks.privileged
    > kubectl apply -f psp/psp_default.yaml
    > kubectl apply -f psp/psp_restricted.yaml
    > kube get psp

14. ECR using VPC private endpoints:
    > To get public Ip address of ecr endpoint: ping api.ecr.us-east-1.amazonaws.com
    > Steps to create VPC endpoint:
      a) Go to VPC endpoint and select your VPC. You will see 2 options: api & dkr endpoints.
      b) aws ecr describe-repositories --region us-east-1 : api interface is used.
      c) docker command is used to build and push image, here drk interface is used. When endpoint is created it will use VPC private link or VPC private channel to push image to ECR repository. 
      d) Create api endpoint: select VPC and private subnet and enable dns name.Create security Group and allow all traffic. Customize your endpoint policy as per requirement.
      e) RUN: ping api.ecr.us-east-1.amazonaws.com - Ip is changed to private Ip and it belongs to subnet.
      f) Create dkr endpoint: select VPC and private subnet and enable dns name.Create security Group and allow all traffic.Customize your endpoint policy as per requirement. Here your private dns name is *.dkr.ecr.us-east-1.amazonaws.com, * will be replaced with accountNumber.
      g) Create S3 endpoint: Select VPC and routetable associated with private-subnet.Customize your endpoint policy as per requirement. It is required because ECR uses S3 to store docker image layers. When you instances download docker image from ECR, They msut access ECR to access image manifest and S3 to download actual image layers.
      h) login to ECR: aws ecr get-login-password --regon us-east-1 | docker login --username AWS --password-stdin <account-number>.dkr.ecr.us-east-1.amazonaws.com
      i) build: docker build -t dev/awscli .
      j) Tag: docker tag dev/awscli:latest <account-number>.dkr.ecr.us-east-1.amazonaws.com/dev/awscli:latest
      k) List Image: docker images
      l) PUSH: docker push <account-number>.dkr.ecr.us-east-1.amazonaws.com/dev/awscli:latest


